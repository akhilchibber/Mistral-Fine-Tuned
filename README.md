# Optimizing the Performance of the Mistral 7B Model
<p align="center">
  <img src="https://github.com/akhilchibber/Mistral-Fine-Tuned/blob/main/Mistral-AI.png?raw=true" alt="earthml Logo">
</p>

## Dataset
The dataset used in this project can be found on Kaggle: [Mistral Model Dataset](https://www.kaggle.com/models/mistral-ai/mistral/frameworks/PyTorch/variations/7b-v0.1-hf/versions/1). 

## Objective
The project aims to fine-tune the Mistral AI model using the "guanaco-llama2-1k" dataset, a subset of the LLaMA 2 dataset, to enhance its text generation capabilities. This process aims to improve the model's understanding, coherence, and contextual relevance in generated text across a broad spectrum of topics.

## Key Components
- **Mistral AI Model**: A large language model being fine-tuned for enhanced performance.
- **guanaco-llama2-1k Dataset**: A curated subset from the LLaMA 2 dataset used for fine-tuning.
- **Hugging Face Transformers**: Utilized for model and tokenizer functionality.
- **WandB**: Used for monitoring the training process and results.

## Functionality
1. Fine-tuning the Mistral AI model on the selected dataset.
2. Monitoring the training process with WandB.
3. Evaluating the fine-tuned model's ability to generate more accurate and contextually relevant text.
<p align="center">
  <img src="https://github.com/akhilchibber/Mistral-Fine-Tuned/blob/main/Fine-Tuning.png" alt="earthml Logo">
</p>

## Use Case
The enhanced model can be applied in various domains requiring nuanced text generation, such as chatbots, content creation, and automated customer service responses. It aims to provide more accurate, coherent, and context-relevant text outputs, improving the user experience in applications leveraging AI-driven text generation.

## Getting Started
To get started with this project:

1. Clone this repository to your local machine.
2. Ensure you have Jupyter Notebook installed and running.
3. Install the required dependencies.
4. Download the "Mistral Model Dataset" and place it in the designated directory.
5. Open and run the Jupyter Notebook "Mistral-Fine-Tuned.ipynb" to train and evaluate the model.

## Contributing
We welcome contributions to enhance the functionality and efficiency of this script. Feel free to fork, modify, and make pull requests to this repository. To contribute:

1. Fork the Project.
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`).
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`).
4. Push to the Branch (`git push origin feature/AmazingFeature`).
5. Open a Pull Request against the `main` branch.

## License

This project is licensed under the MIT License - see the `LICENSE` file for details.

## Contact

Author: Akhil Chhibber

LinkedIn: https://www.linkedin.com/in/akhilchhibber/
